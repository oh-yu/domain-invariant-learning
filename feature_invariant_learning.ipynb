{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b67bcd-5b7a-4730-8567-fa85b45d3eb3",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63f2b61b-b0de-4497-8469-e084b81d4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f981ab7b-288d-4260-8a5b-5f0a60b8db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d7f7d-a0b4-4044-b6e4-936b398c1bfc",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03c6b5bf-09f4-44fb-bbbc-9108418e0e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_source: 100\n",
      "D: 2\n",
      "\n",
      "source_X: (100, 2)\n",
      "source_y_domain: (100, 1)\n",
      "source_y_task: (100, 1)\n",
      "source_Y: (100, 2)\n",
      "\n",
      "N_target: 100\n",
      "target_X: (100, 2)\n",
      "target_y_domain: (100,)\n",
      "target_y_task: (100,)\n"
     ]
    }
   ],
   "source": [
    "# source_X: shape of (N_source, D)\n",
    "# source_y_domain: (N_source, 1)\n",
    "# source_y_task: (N_source, 1)\n",
    "# source_Y: (N_source, 2)\n",
    "\n",
    "# target_X: (N_target, D)\n",
    "# target_y_domain: (N_target, )\n",
    "# target_y_task: (N_target, )\n",
    "\n",
    "source_X, target_X, source_y_task, target_y_task, _, _, _ = utils.get_source_target()\n",
    "source_y_domain = np.zeros_like(source_y_task).reshape(-1, 1)\n",
    "source_y_task = source_y_task.reshape(-1, 1)\n",
    "\n",
    "source_Y = np.concatenate([source_y_task, source_y_domain], axis=1)\n",
    "target_y_domain = np.ones_like(target_y_task)\n",
    "\n",
    "print(f\"N_source: {source_X.shape[0]}\")\n",
    "print(f\"D: {source_X.shape[1]}\\n\")\n",
    "\n",
    "print(f\"source_X: {source_X.shape}\")\n",
    "print(f\"source_y_domain: {source_y_domain.shape}\")\n",
    "print(f\"source_y_task: {source_y_task.shape}\")\n",
    "print(f\"source_Y: {source_Y.shape}\\n\")\n",
    "\n",
    "print(f\"N_target: {target_X.shape[0]}\")\n",
    "print(f\"target_X: {target_X.shape}\")\n",
    "print(f\"target_y_domain: {target_y_domain.shape}\")\n",
    "print(f\"target_y_task: {target_y_task.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15e4fad-6f17-4427-aee0-c9d8d155916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_X = torch.tensor(source_X, dtype=torch.float32)\n",
    "source_Y = torch.tensor(source_Y, dtype=torch.float32)\n",
    "target_X = torch.tensor(target_X, dtype=torch.float32)\n",
    "target_y_domain = torch.tensor(target_y_domain, dtype=torch.float32)\n",
    "target_y_task = torch.tensor(target_y_task, dtype=torch.float32)\n",
    "\n",
    "source_X = source_X.to(device)\n",
    "source_Y = source_Y.to(device)\n",
    "target_X = target_X.to(device)\n",
    "target_y_domain = target_y_domain.to(device)\n",
    "target_y_task = target_y_task.to(device)\n",
    "\n",
    "source_ds = TensorDataset(source_X, source_Y)\n",
    "target_ds = TensorDataset(target_X, target_y_domain)\n",
    "\n",
    "source_loader = DataLoader(source_ds, batch_size=16, shuffle=True)\n",
    "target_loader = DataLoader(target_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ad5c83-498c-4252-baba-e8c08372f64c",
   "metadata": {},
   "source": [
    "# Instantiate Feature Extractor, Domain Classifier, Task Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fc267d5-c0d9-4801-8254-41e9f3724e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, dropout_ratio):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.fc2 = nn.Linear(10, 15)\n",
    "        self.fc3 = nn.Linear(15, output_size)\n",
    "        self.bn1 = nn.BatchNorm1d(10)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3b1b39-35c0-482d-b6ac-c5b980a902f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "num_domains = 1\n",
    "num_classes = 1\n",
    "dropout_ratio = 0.5\n",
    "\n",
    "feature_extractor = MLP(input_size=source_X.shape[1], output_size=hidden_size, dropout_ratio=0.5).to(device)\n",
    "domain_classifier = MLP(input_size=hidden_size, output_size=num_domains, dropout_ratio=0.5).to(device)\n",
    "task_classifier = MLP(input_size=hidden_size, output_size=num_classes, dropout_ratio=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ebe3ac7-f08d-4a3d-b431-eeae50aa958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "feature_optimizer = optim.Adam(feature_extractor.parameters(), lr=learning_rate)\n",
    "domain_optimizer = optim.Adam(domain_classifier.parameters(), lr=learning_rate)\n",
    "task_optimizer = optim.Adam(task_classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482826e7-85ae-45f9-b3e3-3d28ac581741",
   "metadata": {},
   "source": [
    "# Feature Invariant Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea70251-181e-4378-87e3-5240bb692680",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [15, 1]], which is output 0 of TBackward, is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a0dae1812e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_task\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mloss_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mfeature_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [15, 1]], which is output 0 of TBackward, is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "alpha = 0.1\n",
    "# Parameters: source_loader, target_loader, num_epochs\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    for (source_X, source_Y), (target_X, target_y_domain) in zip(source_loader, target_loader):\n",
    "        # 0. Data\n",
    "        source_X = source_X.to(device)\n",
    "        source_y_task = source_Y[:, 0].to(device)\n",
    "        source_y_domain = source_Y[:, 1].to(device)\n",
    "        target_X = target_X.to(device)\n",
    "        target_y_domain = target_y_domain.to(device)\n",
    "\n",
    "        # 1. Forward\n",
    "        # 1.1 Feature Extractor\n",
    "        source_X, target_X = feature_extractor(source_X), feature_extractor(target_X)\n",
    "\n",
    "        # 1.2. Domain Classifier\n",
    "        pred_source_y_domain, pred_target_y_domain = domain_classifier(source_X), domain_classifier(target_X)\n",
    "        pred_source_y_domain, pred_target_y_domain = torch.sigmoid(pred_source_y_domain), torch.sigmoid(pred_target_y_domain)\n",
    "\n",
    "        loss_domain = criterion(pred_source_y_domain.reshape(-1), source_y_domain.reshape(-1))\n",
    "        loss_domain += criterion(pred_target_y_domain.reshape(-1), target_y_domain.reshape(-1))\n",
    "\n",
    "        # 1.3. Task Classifier\n",
    "        pred_y_task = task_classifier(source_X)\n",
    "        pred_y_task = torch.sigmoid(pred_y_task)\n",
    "        loss_task = criterion(pred_y_task.reshape(-1), source_y_task.reshape(-1))\n",
    "\n",
    "        # 2. Backward, Update Params\n",
    "        loss_domain.backward(retain_graph=True)\n",
    "        domain_optimizer.step()\n",
    "\n",
    "        loss_task.backward(retain_graph=True)\n",
    "        task_optimizer.step()\n",
    "\n",
    "        loss_feature = loss_task - alpha * loss_domain\n",
    "        loss_feature.backward()\n",
    "        feature_optimizer.step()\n",
    "        break\n",
    "    # 4. Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
