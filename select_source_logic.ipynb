{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c49f3c-9298-4cea-8016-a5d0a5ecdbe8",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a913270c-7584-4643-b595-42d92700fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e61906-e2d9-46e8-bcd8-1d80d7d3a940",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225fa933-5c76-41f3-b2a0-074926d179ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_X = pd.read_csv(\"./deep_occupancy_detection/data/1_X_train.csv\").values\n",
    "target_X = pd.read_csv(\"./deep_occupancy_detection/data/2_X_train.csv\").values\n",
    "\n",
    "source_y_task = pd.read_csv(\"./deep_occupancy_detection/data/1_Y_train.csv\").values.reshape(-1)\n",
    "target_y_task = pd.read_csv(\"./deep_occupancy_detection/data/2_Y_train.csv\").values.reshape(-1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "source_X = scaler.fit_transform(source_X)\n",
    "target_X = scaler.fit_transform(target_X)\n",
    "\n",
    "source_target_X = np.concatenate([source_X, target_X], axis=0)\n",
    "source_target_y_domain = np.concatenate([np.zeros(source_X.shape[0]), np.ones(target_X.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751351d7-de2d-44cb-a54c-af83fc9b81d7",
   "metadata": {},
   "source": [
    "# 1. Covariate Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7340ec-eff9-443d-bc1b-0c46cb6b275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0         1    2    3    4     5         6         7    8    9   ...   90  \\\n",
      "0  1.0  0.777778  1.0  1.0  1.0  0.25  0.142857  0.777778  0.0  0.0  ...  0.2   \n",
      "1  0.0  0.222222  0.0  0.0  0.0  0.75  0.857143  0.222222  1.0  1.0  ...  0.8   \n",
      "\n",
      "    91   92   93   94        95   96   97   98        99  \n",
      "0  0.1  0.0  0.0  0.2  0.142857  0.7  0.5  0.4  0.666667  \n",
      "1  0.9  1.0  1.0  0.8  0.857143  0.3  0.5  0.6  0.333333  \n",
      "\n",
      "[2 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Transform d-dimentional array into 1-dimentional\n",
    "tsne = TSNE(n_components=1, learning_rate=\"auto\", init=\"random\", perplexity=3)\n",
    "# TODO: Understand Argumetns for t-SNE\n",
    "source_X_tsne = tsne.fit_transform(source_X)\n",
    "\n",
    "# 2. Continuous to categorical\n",
    "num_bins = 100\n",
    "# TODO: Found non-gaussian distribution case, not suitable for standardization\n",
    "source_X_tsne = scaler.fit_transform(source_X_tsne)\n",
    "source_X_tsne, bins = pd.cut(source_X_tsne[:, 0], num_bins, labels=False, retbins=True)\n",
    "\n",
    "# 3. Count y for every unique value of x(â‰’ p(y|x))\n",
    "p_y_of_x_source = pd.DataFrame()\n",
    "for unique_X in range(num_bins):\n",
    "    total = len(source_y_task[source_X_tsne == unique_X])\n",
    "    count_1 = sum(source_y_task[source_X_tsne == unique_X])\n",
    "    count_0 = total - count_1\n",
    "    p_y_of_x_source[unique_X] = [count_0, count_1]\n",
    "p_y_of_x_source /= len(source_y_task)\n",
    "p_y_of_x_source /= p_y_of_x_source.values.sum(axis=0)\n",
    "p_y_of_x_source = p_y_of_x_source.fillna(0)\n",
    "\n",
    "print(p_y_of_x_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43261b24-8abe-49b6-ac31-0a54c189a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0    1    2    3    4    5    6    7    8    9   ...        90   91  \\\n",
      "0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  ...  0.272727  0.8   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.727273  0.2   \n",
      "\n",
      "         92   93   94   95   96   97   98   99  \n",
      "0  0.666667  1.0  1.0  1.0  1.0  0.9  0.0  0.0  \n",
      "1  0.333333  0.0  0.0  0.0  0.0  0.1  1.0  1.0  \n",
      "\n",
      "[2 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Same Process #1. ~ #3. for Target\n",
    "# TODO: Source and Target should have same bins for pd.cut()\n",
    "\n",
    "target_X_tsne = tsne.fit_transform(target_X)\n",
    "target_X_tsne = scaler.fit_transform(target_X_tsne)\n",
    "target_X_tsne = pd.cut(target_X_tsne[:, 0], bins, labels=False)\n",
    "p_y_of_x_target = pd.DataFrame()\n",
    "for unique_X in range(num_bins):\n",
    "    total = len(target_y_task[target_X_tsne == unique_X])\n",
    "    count_1 = sum(target_y_task[target_X_tsne == unique_X])\n",
    "    count_0 = total - count_1\n",
    "    p_y_of_x_target[unique_X] = [count_0, count_1]\n",
    "p_y_of_x_target /= len(target_y_task)\n",
    "p_y_of_x_target /= p_y_of_x_target.values.sum(axis=0)\n",
    "p_y_of_x_target = p_y_of_x_target.fillna(0)\n",
    "print(p_y_of_x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86ad29de-7791-4072-b695-73c987b7eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Distribution Gap MSE: 0.24704834116243604\n"
     ]
    }
   ],
   "source": [
    "# 4. Calculate distribution gap between source and target\n",
    "p_y_of_x_target = p_y_of_x_target.values.reshape(-1)\n",
    "p_y_of_x_source = p_y_of_x_source.values.reshape(-1)\n",
    "mse = sum((p_y_of_x_source-p_y_of_x_target)**2) / (num_bins*2)\n",
    "print(f\"Conditional Distribution Gap MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd8ada-f5b3-434e-9ffc-dd4dbe1ccbc0",
   "metadata": {},
   "source": [
    "# 2. Marginal Distribution Discrepancy between Source and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55de748-3132-4ec0-97fb-750c58b5ec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_target_X = torch.Tensor(source_target_X)\n",
    "source_target_y_domain = torch.Tensor(source_target_y_domain)\n",
    "\n",
    "source_target_X = source_target_X.to(utils.DEVICE)\n",
    "source_target_y_domain = source_target_y_domain.to(utils.DEVICE)\n",
    "\n",
    "source_target_ds = TensorDataset(source_target_X, source_target_y_domain)\n",
    "source_target_loader = DataLoader(source_target_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40f6ca8-0792-416d-ae4f-12fae2f99a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "num_repeats = 10\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d838c1-4d0a-453a-a5ac-56fe6ce60592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Classification Accuracy: 0.7404029846191407\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for _ in range(num_repeats):\n",
    "    domain_classifier = utils.Decoder(input_size=source_target_X.shape[1], output_size=1).to(utils.DEVICE)\n",
    "    optimizer = optim.Adam(domain_classifier.parameters(), lr=0.001)\n",
    "\n",
    "    # TODO: See Convergence, if needed should introduce early stopping\n",
    "    for _ in range(num_epochs):\n",
    "        for source_target_X_batch, source_target_y_domain_batch in source_target_loader:\n",
    "            # Forward\n",
    "            pred_y = domain_classifier(source_target_X_batch)\n",
    "            pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "            loss = criterion(pred_y, source_target_y_domain_batch)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Params\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    pred_y = domain_classifier(source_target_X)\n",
    "    pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "    pred_y = pred_y > 0.5\n",
    "\n",
    "    acc = sum(pred_y == source_target_y_domain) / source_target_y_domain.shape[0]\n",
    "    accs.append(acc.item())\n",
    "\n",
    "print(f\"Domain Classification Accuracy: {np.mean(accs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab0609-f9b6-406d-8b33-ff63f4015795",
   "metadata": {},
   "source": [
    "# 3. Common Model Minimizing Loss of Both Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772b356-5bb5-4920-8eff-084142bd6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_X = torch.Tensor(source_X)\n",
    "target_X = torch.Tensor(target_X)\n",
    "source_y_task = torch.Tensor(source_y_task)\n",
    "target_y_task = torch.Tensor(target_y_task)\n",
    "\n",
    "source_X = source_X.to(utils.DEVICE)\n",
    "target_X = target_X.to(utils.DEVICE)\n",
    "source_y_task = source_y_task.to(utils.DEVICE)\n",
    "target_y_task = target_y_task.to(utils.DEVICE)\n",
    "\n",
    "source_ds = TensorDataset(source_X, source_y_task)\n",
    "target_ds = TensorDataset(target_X, target_y_task)\n",
    "\n",
    "source_loader = DataLoader(source_ds, batch_size=16, shuffle=True)\n",
    "target_loader = DataLoader(target_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "130c172d-99a3-4aec-a05d-89b73ac53bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Model's Cross Entropy Loss: 0.6571887731552124\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for _ in range(num_repeats):\n",
    "    task_classifier = utils.Decoder(input_size=source_X.shape[1], output_size=1).to(utils.DEVICE)\n",
    "    optimizer = optim.Adam(task_classifier.parameters(), lr=0.001)\n",
    "    for _ in range(num_epochs):\n",
    "        for (source_X_batch, source_y_task_batch), (target_X_batch, target_y_task_batch) in zip(source_loader, target_loader):\n",
    "            # Forward\n",
    "            pred_source_y_task = task_classifier(source_X_batch)\n",
    "            pred_target_y_task = task_classifier(target_X_batch)\n",
    "            pred_source_y_task = torch.sigmoid(pred_source_y_task).reshape(-1)\n",
    "            pred_target_y_task = torch.sigmoid(pred_target_y_task).reshape(-1)\n",
    "            loss = criterion(pred_source_y_task, source_y_task_batch)\n",
    "            loss += criterion(pred_target_y_task, target_y_task_batch)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Params\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    pred_y = task_classifier(source_X)\n",
    "    pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "    loss = criterion(pred_y, source_y_task)\n",
    "\n",
    "    pred_y = task_classifier(target_X)\n",
    "    pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "    loss += criterion(pred_y, target_y_task)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Common Model's Cross Entropy Loss: {np.mean(losses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
