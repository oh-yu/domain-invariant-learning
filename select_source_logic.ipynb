{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c49f3c-9298-4cea-8016-a5d0a5ecdbe8",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a913270c-7584-4643-b595-42d92700fbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e61906-e2d9-46e8-bcd8-1d80d7d3a940",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225fa933-5c76-41f3-b2a0-074926d179ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_X = pd.read_csv(\"./deep_occupancy_detection/data/1_X_train.csv\").values\n",
    "target_X = pd.read_csv(\"./deep_occupancy_detection/data/2_X_train.csv\").values\n",
    "\n",
    "source_y_task = pd.read_csv(\"./deep_occupancy_detection/data/1_Y_train.csv\").values.reshape(-1)\n",
    "target_y_task = pd.read_csv(\"./deep_occupancy_detection/data/2_Y_train.csv\").values.reshape(-1)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(source_X)\n",
    "source_X = scaler.transform(source_X)\n",
    "target_X = scaler.transform(target_X)\n",
    "\n",
    "source_target_X = np.concatenate([source_X, target_X], axis=0)\n",
    "source_target_y_domain = np.concatenate([np.zeros(source_X.shape[0]), np.ones(target_X.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36af3035-f321-456a-8a70-3bdbe836d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2. Marginal Distribution Discrepancy between Source and Target\n",
    "source_target_X = torch.Tensor(source_target_X)\n",
    "source_target_y_domain = torch.Tensor(source_target_y_domain)\n",
    "\n",
    "source_target_X = source_target_X.to(utils.DEVICE)\n",
    "source_target_y_domain = source_target_y_domain.to(utils.DEVICE)\n",
    "\n",
    "source_target_ds = TensorDataset(source_target_X, source_target_y_domain)\n",
    "source_target_loader = DataLoader(source_target_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24b09dc-7578-41c1-9f9d-d3cbd9ed5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 3. Common Model Minimizin Loss of Both Domains\n",
    "source_X = torch.Tensor(source_X)\n",
    "target_X = torch.Tensor(target_X)\n",
    "source_y_task = torch.Tensor(source_y_task)\n",
    "target_y_task = torch.Tensor(target_y_task)\n",
    "\n",
    "source_X = source_X.to(utils.DEVICE)\n",
    "target_X = target_X.to(utils.DEVICE)\n",
    "source_y_task = source_y_task.to(utils.DEVICE)\n",
    "target_y_task = target_y_task.to(utils.DEVICE)\n",
    "\n",
    "source_ds = TensorDataset(source_X, source_y_task)\n",
    "target_ds = TensorDataset(target_X, target_y_task)\n",
    "\n",
    "source_loader = DataLoader(source_ds, batch_size=16, shuffle=True)\n",
    "target_loader = DataLoader(target_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751351d7-de2d-44cb-a54c-af83fc9b81d7",
   "metadata": {},
   "source": [
    "# 1. Covariate Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e53349d-11f2-42fa-aeca-ddf55436738f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source percentages of variance explanation: [9.99999151e-01 8.48790585e-07]\n",
      "   0   1   2   3    4    5    6   7   8   9   ...  90  91  92  93  94  95  96  \\\n",
      "0   0   0   0  13  210  147   26   8   4   3  ...   0   0   0   0   0   0   0   \n",
      "1   2   1   5  13  169  465  162  63  30  19  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "   97  98  99  \n",
      "0   0   0   0  \n",
      "1   0   0   1  \n",
      "\n",
      "[2 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Transform d-dimentional array into 1-dimentional\n",
    "pca = PCA(n_components=5, svd_solver=\"full\")\n",
    "# TODO: Understand svd_solver=\"full\"(i.e. workaround for ValueError https://stackoverflow.com/questions/41230558/pca-in-sklearn-valueerror-array-must-not-contain-infs-or-nans)\n",
    "# TODO: Understand params for PCA\n",
    "source_X = pca.fit_transform(source_X)\n",
    "print(f\"Source percentages of variance explanation: {pca.explained_variance_ratio_}\")\n",
    "# TODO: Understand PCA Algo\n",
    "\n",
    "# 2. Continuous to categorical\n",
    "num_bins = 100\n",
    "source_X, bins = pd.cut(source_X[:, 0], num_bins, labels=False, retbins=True)\n",
    "\n",
    "# 3. Count y for every unique value of x(â‰’ p(y|x))\n",
    "p_y_of_x_source = pd.DataFrame()\n",
    "for unique_X in range(num_bins):\n",
    "    total = len(source_y_task[source_X == unique_X])\n",
    "    count_1 = sum(source_y_task[source_X == unique_X])\n",
    "    count_0 = total - count_1\n",
    "    p_y_of_x_source[unique_X] = [count_0, count_1]\n",
    "print(p_y_of_x_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43261b24-8abe-49b6-ac31-0a54c189a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target percentages of variance explanation: [9.99331916e-01 2.74240981e-04]\n",
      "   0   1   2   3   4    5   6   7   8   9   ...  90  91  92  93  94  95  96  \\\n",
      "0   0   0   0   0   0  643   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "1   0   0   0   0   0  925   0   0   0   0  ...   0   0   0   0   0   0   0   \n",
      "\n",
      "   97  98  99  \n",
      "0   0   0   0  \n",
      "1   0   0   0  \n",
      "\n",
      "[2 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Same Process #1. ~ #3. for Target\n",
    "# TODO: Source and Target should have same bins for pd.cut()\n",
    "\n",
    "pca = PCA(n_components=5, svd_solver=\"full\")\n",
    "target_X = pca.fit_transform(target_X)\n",
    "print(f\"Target percentages of variance explanation: {pca.explained_variance_ratio_}\")\n",
    "target_X = pd.cut(target_X[:, 0], bins, labels=False)\n",
    "p_y_of_x_target = pd.DataFrame()\n",
    "for unique_X in range(num_bins):\n",
    "    total = len(target_y_task[target_X == unique_X])\n",
    "    count_1 = sum(target_y_task[target_X == unique_X])\n",
    "    count_0 = total - count_1\n",
    "    p_y_of_x_target[unique_X] = [count_0, count_1]\n",
    "print(p_y_of_x_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86ad29de-7791-4072-b695-73c987b7eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Distribution Gap MSE: 2814.78\n"
     ]
    }
   ],
   "source": [
    "# 4. Calculate distribution gap between source and target\n",
    "p_y_of_x_target = p_y_of_x_target.values.reshape(-1)\n",
    "p_y_of_x_source = p_y_of_x_source.values.reshape(-1)\n",
    "mse = sum((p_y_of_x_source-p_y_of_x_target)**2) / (num_bins*2)\n",
    "print(f\"Conditional Distribution Gap MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd8ada-f5b3-434e-9ffc-dd4dbe1ccbc0",
   "metadata": {},
   "source": [
    "# 2. Marginal Distribution Discrepancy between Source and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40f6ca8-0792-416d-ae4f-12fae2f99a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "num_repeats = 10\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d838c1-4d0a-453a-a5ac-56fe6ce60592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Classification Accuracy: 0.7404029846191407\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "for _ in range(num_repeats):\n",
    "    domain_classifier = utils.Decoder(input_size=source_target_X.shape[1], output_size=1).to(utils.DEVICE)\n",
    "    optimizer = optim.Adam(domain_classifier.parameters(), lr=0.001)\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        for source_target_X_batch, source_target_y_domain_batch in source_target_loader:\n",
    "            # Forward\n",
    "            pred_y = domain_classifier(source_target_X_batch)\n",
    "            pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "            loss = criterion(pred_y, source_target_y_domain_batch)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Params\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    pred_y = domain_classifier(source_target_X)\n",
    "    pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "    pred_y = pred_y > 0.5\n",
    "\n",
    "    acc = sum(pred_y == source_target_y_domain) / source_target_y_domain.shape[0]\n",
    "    accs.append(acc.item())\n",
    "\n",
    "print(f\"Domain Classification Accuracy: {np.mean(accs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab0609-f9b6-406d-8b33-ff63f4015795",
   "metadata": {},
   "source": [
    "# 3. Common Model Minimizing Loss of Both Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "130c172d-99a3-4aec-a05d-89b73ac53bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Model's Cross Entropy Loss: 0.6571887731552124\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for _ in range(num_repeats):\n",
    "    task_classifier = utils.Decoder(input_size=source_X.shape[1], output_size=1).to(utils.DEVICE)\n",
    "    optimizer = optim.Adam(task_classifier.parameters(), lr=0.001)\n",
    "    for _ in range(num_epochs):\n",
    "        for (source_X_batch, source_y_task_batch), (target_X_batch, target_y_task_batch) in zip(source_loader, target_loader):\n",
    "            # Forward\n",
    "            pred_source_y_task = task_classifier(source_X_batch)\n",
    "            pred_target_y_task = task_classifier(target_X_batch)\n",
    "            pred_source_y_task = torch.sigmoid(pred_source_y_task).reshape(-1)\n",
    "            pred_target_y_task = torch.sigmoid(pred_target_y_task).reshape(-1)\n",
    "            loss = criterion(pred_source_y_task, source_y_task_batch)\n",
    "            loss += criterion(pred_target_y_task, target_y_task_batch)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Params\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    pred_y = task_classifier(source_X)\n",
    "    pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "    loss = criterion(pred_y, source_y_task)\n",
    "\n",
    "    pred_y = task_classifier(target_X)\n",
    "    pred_y = torch.sigmoid(pred_y).reshape(-1)\n",
    "    loss += criterion(pred_y, target_y_task)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Common Model's Cross Entropy Loss: {np.mean(losses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
